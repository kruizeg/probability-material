% arara: pdflatex: { shell: yes }
% arara: pythontex: {verbose: yes, rerun: modified }
% arara: pdflatex: { shell: yes }

\input{header.tex}
\chapter{Chapter 3: Exercises and remarks}
\textbf{Probabilities are functions from random events to numbers within [0,1]. Random variables are also functions from the collection of outcomes to a set of numbers!} 
We can then \textbf{use random variables to denote random events},
e.g., $\{X = 2\}$ denotes the event containing all outcomes such that X would map those outcomes to value 2. Once we
have linked random variables with random events, we can assign probabilities to random variables via the probabilities of random events generated by these random variables, e.g., random event $\{X\leq a\}$ and its probability value $\P{X\leq a}$.\\~\\
The connection between random variables and probabilities is key in probability theory studies. When we talk about the distribution of a random variable, we are
talking about the probability value of all events associated with the random variable. Luckily, to determine all probability
values of these random events generated by random variables, we only need to know the probabilities of events of certain types, and the rest
can be inferred from these probabilities. For example, you can calculate various probability values of random events generated by random variables as long as you know the CDF, $F(a)=P\{X\leq a \}$, for real valued random variables. For discrete random variables we can also consider the PMF.

\section{Random variables}
\label{sec:section-3.1}

\begin{exercise}\label{ex:chap03:01}
	Consider two independent coin tosses. Denote by $H_i$ and $T_i$ the event that the $i$th coin lands heads or tails, respectively. The coin is not fair. Write $p$ for the probability of a coin landing heads, i.e. $\P{H_i} = p$. Denote $X$ the number of heads in two throws.
	\begin{enumerate}
		\item What is the support of $X$?
		\item For all $s \in S$ in the sample space $S = \{H_1H_2, H_1T_2, T_1H_2, T_1T_2\}$, calculate $X(s)$.
		\item  For all $s\in S$ with the sample space $S=\{T_1T_2, H_1H_2, H_1T_2 ~\textit{or}~ T_1H_2\}$, calculate $X(s)$.
		\item List the elements of $\{X = 1\}$.
		\item List the elements of $\{X =X(H_1T_2) \}$.
	\end{enumerate}
		
	\begin{solution}~
		\begin{enumerate}
			\item The number of heads in two throws can take on the values 0, 1 and 2. Therefore, the support of $X$ is given by $\supp(X) = \{0,1,2\}$.	
			\item We have $X(H_1H_2) = 2$, $X(H_1T_2) = X(T_1H_2) = 1$ and $X(T_1T_2) = 0$.
			\item $X(H_1T_2 ~\textit{or}~ T_1H_2) = 1$
			\item $\{X = 1\} = \{H_1 T_2, T_1 H_2\}$
			\item $\{X = X(H_1 T_2)\} = \{X = 1\} = \{H_1 T_2, T_1 H_2\}$
		\end{enumerate}
	\end{solution}
\end{exercise}
	 
\begin{remark}
	Ex \ref{ex:chap03:01} links random variables with events. If you forgot the definition of events, outcomes or sample spaces, revisit Chapter 1.
\end{remark}
	
\section{Distributions, PMF, and CDF}
\label{sec:section-3.2}
	
\begin{exercise}\label{ex:chap03:02} 
	Consider two independent coin tosses. Denote by $H_i$ and $T_i$ the event that the $i$th coin lands heads and tails, respectively. The coin is not fair. Write $p$ for the probability of a coin landing heads, i.e. $\P{H_i} = p$. Denote $X$ the number of heads in two throws and define a function, denoted by $\P{H_1T_2|X}$, that maps $s \in S = \{H_1H_2, H_1T_2, T_1H_2, T_1T_2\}$ to the conditional probability value $\P{H_1T_2|\{X = X(s)\}}$: $\P{H_1T_2|X}(s)=\P{H_1T_2|\{X = X(s)\}}$.
	\begin{enumerate}
		\item What is $\P{H_1T_2|X}(H_1H_2)$?
		\item Calculate $\P{H_1T_2|X}(s)$ for all $s\in S$.
		\item Is $\P{H_1T_2|X}$ a proper (discrete) random variable?
		\item Derive the CDF and PMF of $\P{H_1T_2|X}$.
	\end{enumerate} 
	\begin{solution}
			\begin{enumerate}
			\item   $\P{H_1T_2|X}(H_1H_2)=\P{H_1T_2|\{X=2\}}= \frac{\P{\{H_1T_2\}\cap \{X=2\}}}{\P{X=2}}=0$.
			\item Calculate $\P{H_1T_2|X}(s)$ for all $s\in S$. Note that the support of X contains only three values: 0, 1 and 2. As long as $s$ is one outcome in the sample space that has either two or zero heads, i.e., $\{T_1T_2\},\{H_1H_2\}$, similarly to the above subquestion, $\P{H_1T_2|X}(s)=0$. Now if $s\in \{X=1\}$, then $\P{H_1T_2|X}(s)= \P{H_1T_2|\{X=1\}}= \P{H_1T_2|\{H_1T_2, H_2T_1\}}=1/2.$
			\item Is $\P{H_1T_2|X}$ a proper (discrete) random variable? Based on the above question, yes, a discrete uniform distributed one with support $\{0,1/2\}$.
			\item Derive the CDF and PMF of $\P{H_1T_2|X}$. It is a \textbf{discrete uniform distributed one with support $\{0,1/2\}$.} CDF and PMF is given in the textbook.
		\end{enumerate} 
	\end{solution}
\end{exercise}
 
\begin{exercise} 
	Use the non-naive definition of probability to prove the properties of PMF.
	\begin{hint}
		A \textbf{probability function (measure)} $P$ maps an event , a (well-constructed) subset $A$ of the sample space $S$ ($A\subseteq S$), to the probability of the event $\P{A}$, a real number within $[0,1]$. It should satisfy the \textit{Axioms of Probability} 
\begin{enumerate}
	\item $\P{\emptyset}=0$, and $\P{S}=1$.
	\item If $A_{1},A_{2},\ldots $ are disjoint (i.e. mutually exclusive, $A_i\cap A_j=\emptyset, i\neq j$) events, then
	\begin{equation*}
		\P{\cup_{i=1}^{\infty}A_{i}} = \sum_{i=1}^{\infty}\P{A_{i}}
	\end{equation*}
\end{enumerate}
	\end{hint}
	\begin{solution}
		\begin{enumerate}
			\item Non-negativity of the PMF is due to the fact that $P$ maps an event to a real number within $[0,1]$. So, always non-negative!
			\item As for the sum to one: assume that $X$ has support $A=\{x_1,x_2....\}$, note that $1=\P{S}=\P{\{X\in A\}\cup S\setminus\{X\in A \}}=\P{X\in A} +\P{S\setminus{X\in A }}=\P{\cup_i \{X=x_i\}} +0=\sum_i \P{X=x_i}$. 
		\end{enumerate}
	\end{solution}
\end{exercise}
 
\section{Distributions: Bern, Bin, DUnif, and HGeom}
\label{sec:section-3.3}
 
\begin{exercise}\label{ex:chap03:03}
	Flip a fair coin $n$ times.
	\begin{enumerate}
		\item Let $H$ denotes the event of a coin landing heads and let $T$ denote the event of a coin landing tails. Can you describe a specific outcome via a sequence of $H$'s and $T$'s?
		\item Are these outcome equally likely? What is the probability of observing a specific sequence, e.g., ``$\underbrace{HTT...HTH}_{\textit{n characters in total}}$''?
		\item Now let $X$ denote the number of heads in $n$ coin flips. How many outcomes are contained in the event $\{X=k\}$ for $k = 0, 1 \hdots, n$?
		\item Now derive the PDF and CDF of $X$, what's its distribution name?
	\end{enumerate}
	\begin{solution}~
		\begin{enumerate}
			\item Yes. For example, the outcome ``the first coin lands heads and the rest land tails" can be represented as $\{H\underbrace{TTT \ldots TT}_{\textit{n - 1 T's in total}}\}$.
			\item Yes, all outcomes are equally likely. The probability that the outcome of the $i$th coin flip matches the $i$th value of a specific sequence is $\frac{1}{2}$. So, the probability of observing, say, $\underbrace{\{HTT...HTH\}}_{\textit{n characters in total}}$ is $\underbrace{\frac{1}{2} \cdot \frac{1}{2} \cdot \ldots \frac{1}{2}}_{n \: times} = \left(\frac{1}{2}\right)^n$ and this probability is the same for any other sequence.
			\item There are $\binom{n}{k}$ outcomes with $k$ heads in $n$ coin flips. Hence the event $\{X=k\}$ contains $\binom{n}{k}$ elements.
			\item There are $\binom{n}{k}$ outcomes with $k$ heads in $n$ coin flips. Each outcome has a probability of $\left(\frac{1}{2}\right)^n$ of occurring. Hence, for $k = 0, 1, \hdots, n$, $\P{X = k} = \binom{n}{k} \left(\frac{1}{2}\right)^n$. Recognize that this is the PMF of the $\Bin{n, 1/2}$ distribution. Hence $X \sim \Bin{n, 1/2}$.
		\end{enumerate}
	\end{solution}
\end{exercise}
	
\begin{remark}
	Ex \ref{ex:chap03:03} shows how the binomial distribution is derived. Note that the choice function $\binom{n}{m}$ is also called the \emph{binomial coefficient}. 
\end{remark}

\begin{exercise}
	Let $X \sim \Bin{n, p}$. Show that $Y = n - X \sim \Bin{n, 1 - p}$.
	\begin{solution}
	 	Let $X \sim \Bin{n, p}$. Let $Y = n - X \sim \Bin{n, 1 - p}$. Then
	 	\begin{equation*}
	 		\P{Y = k} = \P{X = n - k} = \binom{n}{n - k} p^{n - k} (1 - p)^k = \binom{n}{k}	 (1 - p)^k p^{n - k}.
	 	\end{equation*}
	 	This shows that the PMF of $Y$ is the $\Bin{n, 1 - p}$ distribution. Hence $Y \sim \Bin{n, 1 - p}$.
	\end{solution}
\end{exercise}

\begin{exercise}\label{ex:chap03:08}
	There are 100 slips of paper in a hat, each of which has one of the numbers $1,2, \ldots, 100$ written on it, with no number appearing more than once. Five of the slips are drawn, one at a time. Suppose sampling happens with replacement.
	\begin{enumerate}
		\item What is the distribution of how many of the drawn slips have a value of at least $80$ written on them?
		\item What is the distribution of the value of the $i$th draw (for $1 \leq i \leq 5$)?
		\item What is the probability that the number 100 is drawn at least once?
	\end{enumerate}
	\begin{solution}~
	 	\begin{enumerate}
	 		\item The distribution is $\Bin{5, 0.21}$.
	 		\item Let $X_i$ be the value of the $i$th draw. By symmetry, $X_i \sim \DUnif{1, 2, \ldots, 100}$.
	 		\item Let $X_i$ be the value of the $i$th draw. Let $A$ be the event that the number 100 is drawn at least once. Then
	 		\begin{align*}
	 			\P{A} & = 1 - \P{A^c} = 1 - \P{X_1 \neq 100, \ldots, X_5 \neq 100} \\
	 			& = 1 - \P{X_1 \neq 100} \ldots \P{X_1 \neq 100} = 1 - 0.99^5 \\
	 			& \approx 0.049.
	 		\end{align*}
	 	\end{enumerate}
	\end{solution}
\end{exercise}
	 
\begin{exercise}
	Redo Ex \ref{ex:chap03:08}, now sampling \emph{without} replacement. How do your answers change?
 	\begin{solution}~
 		\begin{enumerate}
	 		\item The distribution is now $\text{HGeom}(21, 79, 5)$.
	 		\item Let $X_i$ be the value of the $i$th draw. By symmetry, $X_i \sim \DUnif{1, 2, \ldots, 100}$. (Nothing changes.)
	 		\item Let $X_i$ be the value of the $i$th draw. Let $A$ be the event that the number 100 is drawn at least once. The events $X_1 = 100, \ldots, X_5 = 100$ are now disjoint, so
	 			\begin{equation*}
	 				\P{A} = \P{X_1 = 100} + \hdots + \P{X_5 = 100} = 0.05.
	 			\end{equation*}
	 	\end{enumerate}
 	\end{solution}
\end{exercise}
 	
\begin{exercise}\label{ex32111}
	Let $X$ be a discrete random variable with support $R_X=\{1, 2, \ldots\}$. Suppose the PMF of $X$ is given by $p_X(k) = \frac{1}{2^{k}}$ for $k = 1, 2 , \ldots$.
	\begin{enumerate}
		\item Find and plot the CDF of $X, F_X(x)$.
		\item Find $\P{2<X \leq 5}$.
		\item Find $\P{X>4}$.
	\end{enumerate}
	\begin{hint}
		Recall the formulas for geometric series.
	\end{hint}
	\begin{solution}
		First, note that the provided PMF is valid: $\sum_{k = 1}^{\infty} p_X(k)=\sum_{k=1}^{\infty} \frac{1}{2^{k}} = 1$ using the formula for geometric series.
		\begin{enumerate}
			\item  To find the CDF, note that:
			\begin{itemize}
				\item For $x<1, \quad \quad F_X(x)=0$.
				\item For $1 \leq x<2, \: \: F_X(x)=p_X(1)=\frac{1}{2}$.
				\item For $2 \leq x<3, \: \: F_X(x)=p_X(1)+p_X(2)=\frac{1}{2}+\frac{1}{4}=\frac{3}{4}$.
			\end{itemize}
			Generalizing this, for $x \geq 1$ we have
			\begin{equation*}
				F_{X}(x) = \sum_{k = 1}^{\lfloor x \rfloor} F_X(k) = \sum_{k = 1}^{\lfloor x \rfloor} \frac{1}{2^{\lfloor x \rfloor}} = \frac{2^{\lfloor x \rfloor}-1}{2^{\lfloor x \rfloor}}
			\end{equation*}
			again using the formula for geometric series. Now draw a plot.
			\item To find $\P{2<X \leq 5}$, we can write
			\begin{equation*}
				\P{2<X \leq 5} = F_{X}(5) - F_{X}(2) = \frac{31}{32}-\frac{3}{4}=\frac{7}{32}.
			\end{equation*}
			Alternatively, we can write
			\begin{equation*}
				\P{2<X \leq 5} = P_{X}(3)+P_{X}(4)+P_{X}(5)=\frac{1}{8}+\frac{1}{16}+\frac{1}{32}=\frac{7}{32},
			\end{equation*}
			which gives the same answer.
			\item To find $\P{X>4}$, we can write
			\begin{equation*}
				\P{X>4} = 1 - \P{X \leq 4} = 1-F_{X}(4)=1-\frac{15}{16}=\frac{1}{16}
			\end{equation*}
		\end{enumerate}
	\end{solution}
\end{exercise}

\begin{remark}
	From Ex \ref{ex32111}, we see from the probability values of events of form $\{X\leq a\}$ (CDF!) we can easily calculate probability values of random events of other forms: such as   $\{a_1\leq X\leq a_2\}$, $\{X>b\}$, etcetera.
\end{remark}

\begin{remark}
	Compare Ex \ref{ex32111}.1 and  \ref{ex32111}.2: same question, but different notation. Note the flexibility of using notations.
\end{remark}

\begin{exercise}~\label{ex3211}
	\begin{enumerate}
 		\item Let $X_1,X_2\sim \Bern{0.5}$ be independent. Derive the PMF of $Y=X_1+X_2$. 
 		\item Let $X_1,X_2\sim \Bern{0.5}$ be independent. Derive the PMF of $g(X_1,X_2)$, with $g(x_1,x_2)=x_1+x_2$.	 
 		\item Let $X_1\sim \Bin{n, p}$ and $X_2\sim \Bin{m, p}$ be independent. Derive the PMF of $Y=X_1+X_2$.
 		\item Let $X_1\sim \Bin{n, p}$ and $X_2\sim \Bin{n, s}$ be independent. Derive the PMF of $Y=X_1+X_2$.
 	\end{enumerate}
 	\begin{solution}~
 		\begin{enumerate}
 			\item Let $X_1,X_2\sim \Bern{0.5}$ be independent. Then $Y = X_1 + X_2$ has support $\{0,1,2\}$. By independence, $\P{Y = 0} = \P{X_1 = 0, X_2 = 0} = \P{X_1 = 0} \P{X_2 = 0} = 0.5^2$, $\P{Y = 1} = \P{X_1 = 0, X_2 = 1} + \P{X_1 = 1, X_2 = 0} = \P{X_1 = 0} \P{X_2 = 1} + \P{X_1 = 1} \P{X_2 = 0}= 0.5^2 + 0.5^2 = 0.5$ and $\P{Y = 2} = \P{X_1 = 1, X_2 = 1} = \P{X_1 = 1} \P{X_2 = 1} = 0.5^2$. ($\P{Y = k} = 0$ for $k \notin \{0, 1, 2\}$.)
 			\item The PMF of $g(X_1,X_2)$ is the same as the PMF of $Y$ in the previous subquestion.
 			\item Let $X_1 \sim \Bin{n, p}$ and $X_2 \sim \Bin{m, p}$ be independent. Then, by conditioning on $X_1$ and using the law of total probability, we have
				 \begin{align*}
					 \P{Y = k} & = \P{X_1 + X_2 = k} = \sum_{j = 0}^k \P{X_1 + _2 = k|X_2=j} \P{X = j} = \sum_{j = 0}^k \P{X_2 = k - j} \P{X = j} \\
					 & = \sum_{j = 0}^k \binom{m}{k - j} p^{k - j} (1 - p)^{m - k + j} \cdot \binom{n}{j} p^j (1 - p)^{n - j} = p^k (1 - p)^{n + m - k} \sum_{j = 0}^k \binom{m}{k - j} \binom{n}{j} \\
					 & = \binom{n + m}{k} p^k (1 - p)^{n + m - k}\quad\text{for }k=0,1,\dots,n+m.
				 \end{align*}
 			\item Let $X_1\sim \Bin{n, p}$ and $X_2\sim \Bin{n, s}$ be independent. Let $Y=X_1+X_2$. The support of $Y$ is $\{0, 1, \ldots, 2n\}$. Then $\P{Y = a} = \P{X_1 + X_2 = n} = \sum_{b = 0}^k \P{X_1 = a - b} \P{X_2 = b} = \sum_{b = 0}^k \binom{n}{a - b} \binom{n}{b} p^{a - b} (1 - p)^{n + b - a} s^b (1 - s)^{n - b}$ for $a=0,1,\dots,n+m$.
 		\end{enumerate}
 	\end{solution}
\end{exercise}

\begin{exercise}~
	Roll a fair four-sided dice. Let $X$ denote the result of the roll. Now flip $X$ fair coins. Let $Y$ denote the number of resulting heads. What is the PMF of $Y$?
	\begin{hint}
		Apply the law of total probability.
	\end{hint}
 	\begin{solution}
		Roll a fair four-sided dice. Let $X$ denote the result of the roll. Now flip $X$ coins. Let $Y$ denote the number of resulting heads. Then:
 		\begin{align*}
 			\P{Y = 0} & = \sum_{k = 1}^4 \P{Y = 0 | X = k} \P{X = k} = \frac{1}{4} \sum_{k = 1}^4 \binom{k}{0} \frac{1}{2^k} = \frac{1}{4} \left(\frac{1}{2} + \frac{1}{2^2} + \frac{1}{2^3} + \frac{1}{2^4}\right) = \frac{15}{64} \\
 			\P{Y = 1} & = \sum_{k = 1}^4 \P{Y = 1 | X = k} \P{X = k} = \frac{1}{4} \sum_{k = 1}^4 \binom{k}{1} \frac{1}{2^k} = \frac{1}{4} \left(\frac{1}{2} + 2 \cdot \frac{1}{2^2} + 3 \cdot \frac{1}{2^3} + 4 \cdot \frac{1}{2^4}\right) = \frac{13}{32} \\
 			\P{Y = 2} & = \sum_{k = 1}^4 \P{Y = 2 | X = k} \P{X = k} = \frac{1}{4} \sum_{k = 2}^4 \binom{k}{2} \frac{1}{2^k} = \frac{1}{4} \left(\frac{1}{2^2} + 3 \cdot \frac{1}{2^3} + 6 \cdot \frac{1}{2^4}\right) = \frac{1}{4} \\
 			\P{Y = 3} & = \sum_{k = 1}^4 \P{Y = 3 | X = k} \P{X = k} = \frac{1}{4} \sum_{k = 3}^4 \binom{k}{3} \frac{1}{2^k} = \frac{1}{4} \left(\frac{1}{2^3} + 4 \cdot \frac{1}{2^4}\right) = \frac{3}{32} \\
 			\P{Y = 4} & = \sum_{k = 1}^4 \P{Y = 4 | X = k} \P{X = k} = \frac{1}{4} \sum_{k = 4}^4 \binom{k}{4} \frac{1}{2^k} = \frac{1}{4} \cdot \frac{1}{2^4} = \frac{1}{64}3,
 		\end{align*} 
		and zero elsewhere.
 	\end{solution}
\end{exercise}

\begin{exercise}~
	Let $X$ and $Y$ be independent random variables that follow a discrete uniform distribution on the set $\{0, 1, 2\}$ (that is, $X,Y \sim \text{DUnif}(0,1,2)$). Find the CDF of $Z = \max\{X, Y\}$.
 	\begin{solution}
 		Let $X$ and $Y$ be independent random variables that follow a discrete uniform distribution on the set $\{0, 1, 2\}$ (that is, $X,Y \sim \text{DUnif}(0,1,2)$). Let $Z = \max\{X, Y\}$. The support of $Z$ is $\{0, 1, 2\}$. By independence, $\P{X = x, Y = y} = \P{X = x} \P{Y = y}$ for all $x, y \in \{0,1,2\}$. Hence:
		\begin{align*}
 			\P{Z = 0} & = \P{\max\{X, Y\} = 0} = \P{X = 0} \P{Y = 0} = \frac{1}{3} \cdot \frac{1}{3} = \frac{1}{9} \\
 			\P{Z = 1} & = \P{\max\{X, Y\} = 1} = \P{X = 0, Y = 1} + \P{X = 1, Y = 0} + \P{X = 1, Y = 1} = \frac{1}{3} \\
 			\P{Z = 2} & = \P{\max\{X, Y\} = 2} = \P{X = 0, Y = 2} + \P{X = 1, Y = 2} \\
 			& \quad + \P{X = 2, Y = 0} + \P{X = 2, Y = 1} + \P{X = 2, Y = 2} = \frac{5}{9}
 		\end{align*}
 	\end{solution}
\end{exercise}

\begin{exercise}\label{ex:chap03:10}
 	Let $X \sim \Bin{n, p}$ and $Y \sim \Bin{m, p}$ be independent. Find the distribution of $X + Y$.
 	\begin{hint}
 		Condition on $X$ and apply the law of total probability. Or, you can also use the story proof, think about the background story of the binomial distribution.
 	\end{hint}
	\begin{solution}
		Let $X \sim \Bin{n, p}$ and $Y \sim \Bin{m, p}$ be independent. Then, by conditioning on $X$ and using the law of total probability, we have
		\begin{align*}
			\P{X + Y = k} & = \sum_{j = 0}^k \P{X + Y = k} \P{X = j} = \sum_{j = 0}^k \P{Y = k - j} \P{X = j} \\
			& = \sum_{j = 0}^k \binom{m}{k - j} p^{k - j} (1 - p)^{m - k + j} \cdot \binom{n}{j} p^j (1 - p)^{n - j} = p^k (1 - p)^{n + m - k} \sum_{j = 0}^k \binom{m}{k - j} \binom{n}{j} \\
			& = \binom{n + m}{k} p^k (1 - p)^{n + m - k}.
		\end{align*}
		This is the PMF of the $\Bin{n + m, p}$ distribution. Hence, $X + Y \sim \Bin{n + m, p}$. 	(We used the result of Ex \ref{ex:chap01:12} to write $\sum_{j = 0}^k \binom{m}{k - j} \binom{n}{j}$ as $\binom{n + m}{k}$.)
	\end{solution}
\end{exercise}

\begin{remark}
	The word ``distribution'', if you check a dictionary you might find one of its possible interpretations is ``the way in which something is shared out among a group or spread over an area.'' In this course, we did not formally define this word, but when we talk about it, we are talking about how the probability values are assigned to events generated by a given random variable. Hence, to find a distribution of a given random variable, you may use PDF/PMF or CDF or MGF or even the names of a distribution. For example, you can simply say the distribution of $X+Y$ in  Ex \ref{ex:chap03:10} is Bin$(n+m,p)$ (of course, you need provide additional arguments to support your statement).   
\end{remark}

 \begin{exercise}
 	Let $X \sim \Bin{n, p}$ and $Y \sim \Bin{m, p}$ be independent. Find conditional distribution of $X$ given $X + Y = r$.
	\begin{hint}
		Apply Bayes' rule.
	\end{hint}
	\begin{solution}
		Let $X \sim \Bin{n, p}$ and $Y \sim \Bin{m, p}$ be independent. Let $Z = X + Y$. Note that $X + Y \sim \Bin{n+m, p}$ by the result of Ex \ref{ex:chap03:10}. Then
		\begin{align*}
			\P{X = x | X + Y = r} & = \frac{\P{X + Y = r | X = x} \P{X = x}}{\P{X + Y = r}} = \frac{\P{Y = r - x} \P{X = x}}{\P{x + Y = r}} \\
			& = \frac{\binom{m}{r - x} p^{r - x} (1 - p)^{m - r + x} \cdot \binom{n}{x} p^x (1 - p)^{n - x}}{\binom{n + m}{r} p^r (1 - p)^{n + m - r}} \\
			& = \frac{\binom{n}{x} \binom{m}{r - x}}{\binom{n + m}{r}}.
		\end{align*}
		This is the PMF of the $\text{HGeom}(n, m, r)$ distribution. Hence, $X | X + Y = r \sim \text{HGeom}(n, m, r)$.
	\end{solution}
\end{exercise}

 \begin{exercise}
 	Let $X \sim \text{HGeom}(w, b, n)$ and $N = w + b \to \infty$ such that $p = \frac{w}{w + b}$ remains fixed. Show that the PMF of $X$ converges to the PMF of the $\Bin{n, p}$ distribution.
	\begin{hint}
		Use the fact that $X \sim \text{HGeom}(w, b, n)$ and $Y \sim \text{HGeom}(n, w + b - n, w)$ have the same distribution.
	\end{hint}
	\begin{solution}
		Let $X \sim \text{HGeom}(w, b, n)$ and $N = w + b \to \infty$ such that $p = \frac{w}{w + b}$ remains fixed. Then:
		\begin{align*}
			P(X = k) & = \frac{\binom{w}{k} \binom{b}{n - k}}{\binom{w + b}{n}} = \binom{n}{k} \frac{\binom{w + b - n}{w - k}}{\binom{w + b}{w}} = \binom{n}{k} \frac{w!}{(w - k)!} \frac{b!}{(b - n + k)!} \frac{(w + b - n)!}{(w + b)!} \\
			& = \binom{n}{k} \frac{w(w - 1) \hdots (w - k + 1) \cdot b(b - 1) \hdots (b - n + k + 1)}{(w + b)(w + b - 1) \hdots (w + b - n + 1)} \\
			& = \binom{n}{k} \frac{p \left(p - \frac{1}{N}\right) \hdots \left(p - \frac{k - 1}{N}\right) \cdot (1 - p) \left((1 - p) - \frac{1}{N}\right) \hdots \left((1 - p) - \frac{n - k - 1}{N}\right)}{\left(1 - \frac{1}{N}\right) \left(1 - \frac{2}{N}\right) \hdots \left(1 - \frac{n - 1}{N}\right)} \\
			& \to \frac{n}{k} p^k (1 - p)^{n - k} \quad \text{as } N \to \infty
		\end{align*}
		This is the PMF of the $\Bin{n, p}$ distribution. Note here that we used the equivalency of the $\text{HGeom}(w, b, n)$ and $\text{HGeom}(n, w + b - n, w)$ distributions.
	\end{solution}
\end{exercise}






\input{trailer.tex}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "study-guide.tex"
%%% End:
