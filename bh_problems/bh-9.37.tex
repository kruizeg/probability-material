\input{header.tex}
\setcounter{theorem}{36}
\begin{exercise} BH.9.37

Bootstrapping is used in statistics to, for instance, construct confidence intervals. It is a much used and intuitive technique.

Extra exercise to  help you recall some ideas of Ch 1. How many different bootstrap samples are possible?

I used some extra ideas to save some time. We say that the rvs $\{X_i\}$ are independent and  distributed as the common rv~$X$ when $X_i\sim F_X$ where $F_X$ is the CDF of the rv $X$. Then $\E{X_{i}}=\E X$, and so on.  Next,  I prefer to write $Y_{j}= X_j^{*}$, as this writes (and types) faster. Finally,  it is easy to define
$Y_j = \sum_{i=1}^n X_i\1{S_j = i}$, where $S_{j}\sim \DUnif{\{1, \ldots, n\}}$ is the \(j\)th sample of the $\{X_{i}\}$.
 \begin{solution}

a. Here you should assume that the $X_i$ are not yet known. Thus, the expectation over $X_i$ is taken with respect to the CDF $F_X$. Using the independence of $X_j$ and $S_j$, $\1{S_j=i}\1{S_j=k} = 0$ if $i\neq k$, and that $\E{\1{S_j=k}} = 1/n$,
\begin{align*}
\E{Y_j} &= \sum_i \E{X_i}\E{\1{S_{j}=i}} = \mu, \\
\E{Y_j^2}  &= \E{\sum_k\sum_{l} X_k X_{l} \1{S_j=k}\1{S_{j}=l}}
= \E{\sum_k X_k^{2}  \1{S_j=k}} =\sum_{k} \E{X^{2}} n^{-1} = \E{X^{2}}, \\
\V{Y_j}&= \E{Y_j^{2}}- (\E{Y_{j}})^{2} = \sigma^{2}.
\end{align*}

b. Now we are given the outcomes (samples) $X_i=x_i$ of $n$ experiments.
I prefer to write $D = X_{1}, \ldots, X_{n}$ as it is shorter.
Noting that $S_j$ and $D$ are independent, and that $\E{X_{k}|D} = X_{k}$,
\begin{align*}
  \E{Y_{j}|D} = \sum_k X_k \E{\1{S_j=k}|D} = \frac 1 n \sum_k X_k := \bar X
\end{align*}
Observe that this average  need  not be the same as $\mu$!

The conditional variance. Since $S_j$ and $S_k$ are independent when $j\neq k$, it must be that $Y_j|D$ and $Y_k|D$ are also conditionally independent. Moreover, $\{Y_j|D\}$ are conditionally iid. Therefore,
\begin{align*}
\E{Y_{j}^{2}|D}
&= \E{\sum_k\sum_l X_kX_l\1{S_j=k}\1{S_j=l}|D} \\
&= \E{\sum_kX_k^{2}\1{S_j=k}|D}  = \sum_kX_k^{2}\E{\1{S_j=k}|D} \\
&= \frac 1 n \sum_{k} X_k^{2}, \\
\V{Y_j| D} &= \frac 1 n \sum_{k} X_k^{2} - (\bar X)^{2} = \frac 1 n \sum_{k} (X_k - \bar X)^{2} = \frac{n-1}n \sigma^{2},\\
\V{\bar Y|D} &= \V{\frac 1 n\sum_{j}Y_{j}|D} = \frac 1{n^{2}} \sum_j \V{Y_j|D} = \frac 1 n \V{Y_1|D}.
\end{align*}


c. For $\E{\bar Y}$ use linearity and Adam's law:
\begin{align*}
\E{\bar Y} = \E{\E{\bar Y|D}} = \frac 1 n \sum_{k}\E{X_{k}} = \E X = \mu.
\end{align*}


Here are the details for $\V{\bar Y}$. Using  BH.6.3.3 and BH.6.3.4,
\begin{align*}
\E{\V{\bar Y|D}} &=  \frac 1 n \E{\V{Y_{1}|D}} =
\frac 1 {n^2} \E{ \sum_{i=1}^n (X_i-\bar X)^2} \\
&= \frac{n-1} {n^2} \E{\frac 1 {n-1} \sum_{i=1}^n (X_i-\bar X)^2}
= \frac{n-1} {n^2} \E{S_{n}^2} = \frac{(n-1)\sigma^{2}} {n^2} \\
\V{\E{\bar Y|D}} &= \V{\bar X}= \frac 1{n^{2}} \sum_{i} \V{X_{i}} = \frac 1 n \sigma^{2}.
\end{align*}
Now use Eve's law to add both terms to get $V{\bar Y}$.


d. We add randomness twice, first we draw  samples to get $D$, and then we draw randomly from $D$.

The extra exercise: immediate from Example 1.4.22. We are not interested in the sequence of the bootstrap sample. BTW, the story that goes for me with this example is the `balls and bars story'. I have $n$ balls to distribute over $k$ boxes. Hence, there are $k-1$ bars to separate the boxes. For the bootstrap sample, I have to distribute $n$ bootstrap samples (the $X^*_{i}$) over $n$ boxes (the initial sample $X_i$.)

If $n$ is small, say $n=4$. Does it make sense to take more than 1000 bootstrap samples?
\end{solution}
\end{exercise}
\input{trailer.tex}
