\input{header}
\setcounter{theorem}{9}
\begin{exercise}[BH.7.10]
Don't forget to compare  this exercise to BH.7.9, which is the same but for discrete memoryless rvs.

You should use that when two continuous rvs $X$ and $Y$ are independent, then $\P{Y\leq y | X=x} = \P{Y\leq y}$.

\begin{solution}
Using the idea in the problem statement,
\begin{align*}
F_T(t|x) &= \P{T\leq t|X=x} = \P{X+Y\leq t|X=x} \stackrel{1}=\P{Y\leq t-x} \\
&=1 - e^{-\lambda(t-x)}, t \geq x.
\end{align*}
where step 1 follows from the fact that $Y$ and $X$ are independent. When $t<x$, $F_{T|X}(t|x) = 0$.

b. Take the derivative wrt to $t$: $f_{T|X} (t|x) = F_{T|X}'(t|x) = \lambda e^{-\lambda (t-x)}\1{t\geq x\geq 0}$. To see that $f_{T|X}$ a valid PDF, note first that it is positive. Second, it must integrate to 1. By a change of variable,
\begin{align*}
\int_0^{\infty}  f_{T|X} (t|x) \d t = \int_x^{\infty} \lambda e^{-\lambda t} \d x = 1.
\end{align*}
This last equality must be evident now. (Hint, what is the density of a rv  that is $\sim \Exp{\lambda}$?)


c. By the above,
\begin{align*}
f_{T,X}(t,x)  &=  f_X(x)f_Y(t-x) \1{0\leq x \leq t} = \lambda^2 e^{-\lambda x} e^{-\lambda(t-x)} \1{0\leq x \leq t} = \lambda^2e^{-\lambda t} \1{0\leq x\leq t},\\
  &\implies  \\
  f_{X|T}(x|t) &= \frac{f_{T,X}(t,x)}{f_T(t)}  = \frac{\lambda^2e^{-\lambda t} \1{0\leq x\leq t}}{f_T(t)}.
\end{align*}

Now we use some real nice, but subtle, reasoning that allows us to avoid the computation $f_T$ by means of marginalizing out $x$ from $f_{T,X}(t, x)$.
Observe that the LHS is a function of $x$, but the RHS is a constant as long as $x\in [0, t]$.  In other words, the RHS does not depend on $x$ on this interval.
But, on the other hand,  $f_{X|T}(x|t)$ is  a real PDF.
This implies that the constant $f_T(t)$ (since it does not depend on $x$) must be such that $f_{X|T}(x|t)$ integrates to $1$ on $0\leq x \leq t$.
The only possibility is that
\begin{equation*}
f_{X|T}(x|t) = \frac{1}{t}\1{0\leq x \leq t}.
\end{equation*}

This reasoning gives some  offspin.  We can conclude that
\begin{align*}
  f_T(t) = \frac{f_{TX}(t, x)}{f_{T|X}(t,x)}= \lambda^2 t e^{-\lambda t}.
\end{align*}


This is more than a nice trick. Recall it, as it is not only used more often in the book, but also in more advanced courses on data science and machine learning.
\end{solution}
\end{exercise}
\input{trailer}